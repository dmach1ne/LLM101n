{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbbb1cc93fe3e1c",
   "metadata": {},
   "source": [
    "# Assignment 4\n",
    "\n",
    "In this assignment, you will refactor the entire code to PyTorch, making it more modular and efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1e201da9092e12",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9bef55bdd3e6f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import wandb\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def set_seed(seed: int):\n",
    "    \"\"\"\n",
    "    Set the random seed for reproducibility.\n",
    "\n",
    "    Args:\n",
    "        seed (int): The seed value to set.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    print(f\"Random seed set to {seed}\")\n",
    "\n",
    "\n",
    "def configure_device() -> torch.device:\n",
    "    \"\"\"\n",
    "    Configure the device for training.\n",
    "\n",
    "    Returns:\n",
    "        torch.device: The device to use for training.\n",
    "    \"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        num_gpu = torch.cuda.device_count()\n",
    "        print(f\"Running on {num_gpu} {torch.cuda.get_device_name()} GPU(s)\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "        print(f\"Running on {device}\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(f\"Running on {device}\")\n",
    "    return device\n",
    "\n",
    "\n",
    "def load_text(file_path: str, encoding: str = 'utf-8') -> str:\n",
    "    \"\"\"\n",
    "    Load and read text data from a file.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the text file.\n",
    "        encoding (str, optional): File encoding. Defaults to 'utf-8'.\n",
    "\n",
    "    Returns:\n",
    "        str: The content of the text file.\n",
    "    \"\"\"\n",
    "    if not os.path.isfile(file_path):\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "\n",
    "    with open(file_path, 'r', encoding=encoding) as f:\n",
    "        text = f.read()\n",
    "\n",
    "    print(f\"Loaded text data from {file_path} (length: {len(text)} characters).\")\n",
    "    return text\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366f4a5e31b9d092",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64a041bfe9980131",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class MLPConfig:\n",
    "    root_dir: str = os.getcwd() + \"/../../\"\n",
    "    dataset_path: str = \"data/names.txt\"\n",
    "    device: torch.device = torch.device('cpu')  # Automatic device configuration\n",
    "\n",
    "    # Tokenizer\n",
    "    vocab_size: int = 0  # Set later\n",
    "    \n",
    "    # Model\n",
    "    context_size: int = 3\n",
    "    d_embed: int = 8\n",
    "    d_hidden: int = 64\n",
    "    \n",
    "    # Training\n",
    "    val_size: float = 0.1\n",
    "    batch_size: int = 32\n",
    "    max_steps: int = 6000  # Max of max_steps = 6421\n",
    "    lr: float = 0.01\n",
    "    val_interval: int = 100\n",
    "    log_interval: int = 100\n",
    "\n",
    "    seed: int = 101"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37b1da5eb884f97",
   "metadata": {},
   "source": [
    "## Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5aac9ba3a2c94cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set to 101\n"
     ]
    }
   ],
   "source": [
    "set_seed(MLPConfig.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8046aa4cb3a6469f",
   "metadata": {},
   "source": [
    "## Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec9748db8884490e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on 1 NVIDIA GeForce RTX 3060 Ti GPU(s)\n"
     ]
    }
   ],
   "source": [
    "MLPConfig.device = configure_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4d6ad274a6fbbb",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9dbce085edefdc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = [chr(i) for i in range(97, 123)]  # all alphabet characters\n",
    "chars.insert(0, \".\")  # Add special token\n",
    "MLPConfig.vocab_size = len(chars)\n",
    "str2idx = {char: idx for idx, char in enumerate(chars)}\n",
    "idx2str = {idx: char for char, idx in str2idx.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576b370aea4c1555",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8cc4922d3b541a53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded text data from /mnt/c/Users/danie/NLP/LLM101n/notebooks/Assignments/../../data/names.txt (length: 228145 characters).\n"
     ]
    }
   ],
   "source": [
    "names = load_text(MLPConfig.root_dir + MLPConfig.dataset_path).splitlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31bf63ac06c3b24c",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f91738376b5a8431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-Val Split\n",
    "train_names, val_names = train_test_split(names, test_size=MLPConfig.val_size, random_state=MLPConfig.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f27ec069c3321f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Size: 28829\n",
      "Validation Size: 3204\n",
      "Train Example: keyler\n",
      "Validation Example: jessamae\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train Size: {len(train_names)}\")\n",
    "print(f\"Validation Size: {len(val_names)}\")\n",
    "print(f\"Train Example: {train_names[0]}\")\n",
    "print(f\"Validation Example: {val_names[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44931589173ddd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(_names):\n",
    "    _inputs, _targets = [], []\n",
    "\n",
    "    for name in _names:\n",
    "        context = [0] * MLPConfig.context_size\n",
    "\n",
    "        for char in name + \".\":\n",
    "            idx = str2idx[char]\n",
    "            _inputs.append(context)\n",
    "            _targets.append(idx)\n",
    "            context = context[1:] + [idx]  # Shift the context by 1 character\n",
    "\n",
    "    _inputs = torch.tensor(_inputs)\n",
    "    _targets = torch.tensor(_targets)\n",
    "\n",
    "    return _inputs, _targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27772dc3e0015a64",
   "metadata": {},
   "source": [
    "### Task 1: PyTorch DataLoader\n",
    "\n",
    "We have been using plain Python lists to and then converted them to PyTorch tensors. This is not efficient since it is loading the entire dataset into memory.\n",
    "\n",
    "PyTorch provides `Dataset` and `DataLoader` class to load the data in memory on the fly. [PyTorch Documentation](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html)\n",
    "\n",
    "Refactor the `prepare_dataset` function into a PyTorch `Dataset` class and use the `DataLoader` to efficiently load the data in batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b550956d3a003a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "class NamesDataset(Dataset):\n",
    "    ################################################################################\n",
    "    # TODO:                                                                        #\n",
    "    # PyTorch Dataset requires 3 methods:                                          #\n",
    "    # __init__ method to initialize the dataset                                    #\n",
    "    # __len__ method to return the size of the dataset                             #\n",
    "    # __getitem__ method to return a sample from the dataset                       #\n",
    "    ################################################################################\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    def __init__(self, _names: List[str], context_size: int):\n",
    "        \"\"\"\n",
    "        Initialize the dataset\n",
    "\n",
    "        Args:\n",
    "            _names (List[str]): List of names\n",
    "            context_size (int): Context size of the model\n",
    "        \"\"\"\n",
    "        self.inputs, self.targets = [], []\n",
    "        for name in _names:\n",
    "            context = [0] * context_size\n",
    "            for char in name + \".\":\n",
    "                idx = str2idx[char]\n",
    "                self.inputs.append(context)\n",
    "                self.targets.append(idx)\n",
    "                context = context[1:] + [idx]\n",
    "                \n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"\n",
    "        Return the number of samples in the dataset\n",
    "\n",
    "        Returns:\n",
    "            (int): Number of samples\n",
    "        \"\"\"\n",
    "\n",
    "        return len(self.inputs)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Return a sample from the dataset\n",
    "\n",
    "        Args:\n",
    "            idx (int): Index of the sample\n",
    "\n",
    "        Returns:\n",
    "            Tuple[torch.Tensor, torch.Tensor]: Input and target tensors\n",
    "        \"\"\"\n",
    "        input_idx = torch.tensor(self.inputs[idx])\n",
    "        target_idx = torch.tensor(self.targets[idx])\n",
    "        return input_idx, target_idx\n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "352be875ddc0fa7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the dataset\n",
    "train_dataset = NamesDataset(train_names, MLPConfig.context_size)\n",
    "val_dataset = NamesDataset(val_names, MLPConfig.context_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "27812caaf7fd7ea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Train Samples: 205456\n",
      "Number of Validation Samples: 22690\n",
      "First train (input, target): (tensor([0, 0, 0]), tensor(11))\n",
      "First validation (input, target): (tensor([0, 0, 0]), tensor(10))\n",
      "Second train (input, target): (tensor([ 0,  0, 11]), tensor(5))\n",
      "Second validation (input, target): (tensor([ 0,  0, 10]), tensor(5))\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of Train Samples: {len(train_dataset)}\")\n",
    "print(f\"Number of Validation Samples: {len(val_dataset)}\")\n",
    "print(f\"First train (input, target): {train_dataset[0]}\")\n",
    "print(f\"First validation (input, target): {val_dataset[0]}\")\n",
    "print(f\"Second train (input, target): {train_dataset[1]}\")\n",
    "print(f\"Second validation (input, target): {val_dataset[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a9e04718940ac55d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# DataLoader\n",
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# Initialize the DataLoader for the training and validation datasets.          #\n",
    "################################################################################\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "train_loader = DataLoader(train_dataset,batch_size=MLPConfig.batch_size,shuffle=True)\n",
    "val_loader = DataLoader(val_dataset,batch_size=MLPConfig.batch_size,shuffle=False)\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "13763508cb10bf45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Shape: torch.Size([32, 3])\n",
      "Target Shape: torch.Size([32])\n",
      "Input: tensor([14,  1, 18])\n",
      "Target: 15\n"
     ]
    }
   ],
   "source": [
    "# Example batch\n",
    "_x, _y = next(iter(train_loader))\n",
    "print(f\"Input Shape: {_x.shape}\")   # (batch_size, context_size)\n",
    "print(f\"Target Shape: {_y.shape}\")  # (batch_size)\n",
    "print(f\"Input: {_x[0]}\")\n",
    "print(f\"Target: {_y[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b0001776a8667d",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3035259e2763490c",
   "metadata": {},
   "source": [
    "### Task 2: MLP Model\n",
    "\n",
    "Initialize the weights of the model using the `Kaiming` initialization.\n",
    "\n",
    "What are other activation functions that can be used instead of `tanh`? What are the advantages and disadvantages? Use different activation functions and compare the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3fbb0e90bc505757",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    ################################################################################\n",
    "    # TODO:                                                                        #\n",
    "    # Define the __init__ and forward methods for the MLP model.                   #\n",
    "    # Use the Kaiming initialization for the weights.                              #\n",
    "    # Use other activation functions instead of tanh and compare the results.      #\n",
    "    ################################################################################\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    def __init__(self, vocab_size, context_size, d_embed, d_hidden):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.linear_gain = 1\n",
    "        self.tanh_gain = 5.0/3.0\n",
    "        self.ReLU_gain = 2**0.5\n",
    "        self.embedding_scale = self.linear_gain/(vocab_size*d_embed)**0.5\n",
    "        self.hidden_scale = self.ReLU_gain / (context_size*d_embed)**0.5\n",
    "        self.output_scale = self.linear_gain/(d_hidden)**0.5\n",
    "        \n",
    "        self.C = nn.Parameter(torch.randn(vocab_size, d_embed))*self.embedding_scale\n",
    "        self.W1 = nn.Parameter(torch.randn(context_size*d_embed, d_hidden))*self.hidden_scale\n",
    "        self.b1 = nn.Parameter(torch.randn(d_hidden))\n",
    "        self.W2 = nn.Parameter(torch.randn(d_hidden, vocab_size))*self.output_scale\n",
    "        self.b2 = nn.Parameter(torch.randn(vocab_size))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x_embed = self.C[x]\n",
    "        x = x_embed.vocab_sizeview(x.size(0),-1)\n",
    "        # 딴걸로 바꾸고 위 가중치도 바꾸기\n",
    "        h = F.relu(x@self.W1+self.b1)\n",
    "        logits = h@self.W2+self.b2\n",
    "        return logits\n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "78374911f6424dbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP()\n",
      "Number of parameters: 91\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "mlp = MLP(MLPConfig.vocab_size, MLPConfig.context_size, MLPConfig.d_embed, MLPConfig.d_hidden)\n",
    "mlp.to(MLPConfig.device) # Move the model to the device\n",
    "print(mlp)\n",
    "print(\"Number of parameters:\", sum(p.numel() for p in mlp.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d6bd917a11788c",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4357b5eacdb2bb5a",
   "metadata": {},
   "source": [
    "### Task 3: Wandb Integration\n",
    "\n",
    "[Weights and Biases](https://wandb.ai/site) is a platform to track your machine learning experiments. It is very useful to log the hyperparameters, metrics, and weights of the model. (We can't use matplotlib every time to visualize the results)\n",
    "\n",
    "Create a free account on Wandb. Initialize the wandb run and log the hyperparameters and metrics.\n",
    "\n",
    "**How to set up WANDB API KEY**\n",
    "- Create an account on Wandb\n",
    "- Go to `wandb.ai` -> `Settings` -> `API Keys` -> `Copy API Key`\n",
    "- Set the API key as an environment variable `WANDB_API_KEY`\n",
    "    - What is an environment variable? How to set it? Google `.env`\n",
    "\n",
    "Note: Do not hardcode the API key in the script. Use environment variables.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4c5c45d3303decfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/dmachine-kyung-hee-university/assignment-04/runs/bimczmcf?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fbe343b1b20>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login(key=os.environ.get(\"WANDB_API_KEY\"))\n",
    "wandb.init(\n",
    "    project=\"assignment-04\",\n",
    "    config={\n",
    "        \"d_embed\": MLPConfig.d_embed,\n",
    "        \"d_hidden\": MLPConfig.d_hidden,\n",
    "        \"lr\": MLPConfig.lr,\n",
    "    },\n",
    "    dir=MLPConfig.root_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a58f16a9e13134",
   "metadata": {},
   "source": [
    "### Task 4: Training\n",
    "\n",
    "Train the model. Change the hyperparameters and configurations. Log the results and analyze it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c0dd7f1c2c2b366f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "        model: nn.Module,\n",
    "        train_loader: DataLoader,\n",
    "        val_loader: DataLoader,\n",
    "        max_steps: int,\n",
    "        lr: float,\n",
    "        val_interval: int,\n",
    "        log_interval: int,\n",
    "        device: torch.device,\n",
    "):\n",
    "    \"\"\"\n",
    "    Train the model for a fixed number of steps.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The model to train.\n",
    "        train_loader (DataLoader): DataLoader for the training data.\n",
    "        val_loader (DataLoader): DataLoader for the validation data.\n",
    "        max_steps (int): Maximum number of steps to train.\n",
    "        lr (float): Learning rate.\n",
    "        val_interval (int): Interval for validation.\n",
    "        log_interval (int): Interval for logging.\n",
    "        device (torch.device): Device to run the model on.\n",
    "    \"\"\"\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    wandb.watch(model, log=\"all\", log_freq=log_interval)\n",
    "    running_loss = 0.0\n",
    "    progress_bar = tqdm(enumerate(train_loader), total=max_steps, desc=\"Training\")\n",
    "\n",
    "    for step in range(1, max_steps + 1):\n",
    "        model.train()\n",
    "        for batch_idx, (train_inputs, train_targets) in progress_bar:\n",
    "            train_inputs, train_targets = train_inputs.to(device), train_targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(train_inputs)\n",
    "            loss = F.cross_entropy(logits, train_targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            progress_bar.set_postfix(loss=f\"{running_loss / step:.4f}\")\n",
    "\n",
    "            if step % val_interval == 0:\n",
    "                model.eval()\n",
    "                val_loss = 0.0\n",
    "                total_samples = 0\n",
    "                with torch.no_grad():\n",
    "                    for val_inputs, val_targets in val_loader:\n",
    "                        val_inputs, val_targets = val_inputs.to(device), val_targets.to(device)\n",
    "                        val_logits = model(val_inputs)\n",
    "                        batch_loss = F.cross_entropy(val_logits, val_targets)\n",
    "                        val_loss += batch_loss.item() * val_inputs.size(0)\n",
    "                        total_samples += val_inputs.size(0)\n",
    "                wandb.log({\"Val Loss\": val_loss / total_samples}, step=step)\n",
    "\n",
    "            if step % log_interval == 0:\n",
    "                wandb.log({\"Train Loss\": running_loss / step}, step=step)\n",
    "\n",
    "            step += 1\n",
    "\n",
    "    progress_bar.close()\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd22ec935ba3f10",
   "metadata": {},
   "source": [
    "Note: Unfortunatley PyTorch does not support infinite DataLoader. The train will stop when it reaches the end of the DataLoader. (max_steps=6421)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "efed6ca46233328d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|                                                                                | 0/6000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "indices should be either on cpu or on the same device as the indexed tensor (cpu)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train(\n\u001b[1;32m      2\u001b[0m     model\u001b[38;5;241m=\u001b[39mmlp,\n\u001b[1;32m      3\u001b[0m     train_loader\u001b[38;5;241m=\u001b[39mtrain_loader,\n\u001b[1;32m      4\u001b[0m     val_loader\u001b[38;5;241m=\u001b[39mval_loader,\n\u001b[1;32m      5\u001b[0m     max_steps\u001b[38;5;241m=\u001b[39mMLPConfig\u001b[38;5;241m.\u001b[39mmax_steps,\n\u001b[1;32m      6\u001b[0m     lr\u001b[38;5;241m=\u001b[39mMLPConfig\u001b[38;5;241m.\u001b[39mlr,\n\u001b[1;32m      7\u001b[0m     val_interval\u001b[38;5;241m=\u001b[39mMLPConfig\u001b[38;5;241m.\u001b[39mval_interval,\n\u001b[1;32m      8\u001b[0m     log_interval\u001b[38;5;241m=\u001b[39mMLPConfig\u001b[38;5;241m.\u001b[39mlog_interval,\n\u001b[1;32m      9\u001b[0m     device\u001b[38;5;241m=\u001b[39mMLPConfig\u001b[38;5;241m.\u001b[39mdevice\n\u001b[1;32m     10\u001b[0m )\n",
      "Cell \u001b[0;32mIn[23], line 34\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, val_loader, max_steps, lr, val_interval, log_interval, device)\u001b[0m\n\u001b[1;32m     32\u001b[0m train_inputs, train_targets \u001b[38;5;241m=\u001b[39m train_inputs\u001b[38;5;241m.\u001b[39mto(device), train_targets\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     33\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 34\u001b[0m logits \u001b[38;5;241m=\u001b[39m model(train_inputs)\n\u001b[1;32m     35\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(logits, train_targets)\n\u001b[1;32m     36\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/anaconda3/envs/LLM101n/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/LLM101n/lib/python3.12/site-packages/torch/nn/modules/module.py:1845\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1842\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[1;32m   1844\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1845\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1847\u001b[0m     \u001b[38;5;66;03m# run always called hooks if they have not already been run\u001b[39;00m\n\u001b[1;32m   1848\u001b[0m     \u001b[38;5;66;03m# For now only forward hooks have the always_call option but perhaps\u001b[39;00m\n\u001b[1;32m   1849\u001b[0m     \u001b[38;5;66;03m# this functionality should be added to full backward hooks as well.\u001b[39;00m\n\u001b[1;32m   1850\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m _global_forward_hooks\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m~/anaconda3/envs/LLM101n/lib/python3.12/site-packages/torch/nn/modules/module.py:1793\u001b[0m, in \u001b[0;36mModule._call_impl.<locals>.inner\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1790\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m BackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[1;32m   1791\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1793\u001b[0m result \u001b[38;5;241m=\u001b[39m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1794\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1795\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m   1796\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1797\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1798\u001b[0m     ):\n\u001b[1;32m   1799\u001b[0m         \u001b[38;5;66;03m# mark that always called hook is run\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[20], line 26\u001b[0m, in \u001b[0;36mMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 26\u001b[0m     x_embed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mC[x]\n\u001b[1;32m     27\u001b[0m     x \u001b[38;5;241m=\u001b[39m x_embed\u001b[38;5;241m.\u001b[39mvocab_sizeview(x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m),\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;66;03m# 딴걸로 바꾸고 위 가중치도 바꾸기\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: indices should be either on cpu or on the same device as the indexed tensor (cpu)"
     ]
    }
   ],
   "source": [
    "train(\n",
    "    model=mlp,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    max_steps=MLPConfig.max_steps,\n",
    "    lr=MLPConfig.lr,\n",
    "    val_interval=MLPConfig.val_interval,\n",
    "    log_interval=MLPConfig.log_interval,\n",
    "    device=MLPConfig.device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b18f523de26ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# Analyze the results                                                          #\n",
    "# What hyperparameters worked well? What activation did you use? etc.          #\n",
    "################################################################################\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "#\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0c069cfb0e2b6f",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b3ab05935f1d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_name(model: nn.Module, context_size: int, decoder: dict, end_id: int, device: torch.device) -> str:\n",
    "    \"\"\"\n",
    "    Generate a name using the model.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): Model to generate the name.\n",
    "        context_size (int): Context size of the model.\n",
    "        decoder (dict): Decoder dictionary to convert indices to characters.\n",
    "        end_id (int): End token id.\n",
    "        device (torch.device): Device to run the model on\n",
    "\n",
    "    Returns:\n",
    "        (str): Generated name\n",
    "    \"\"\"\n",
    "    new_name = []\n",
    "    context = [end_id] * context_size\n",
    "\n",
    "    while True:\n",
    "        x = torch.tensor(context).unsqueeze(0).to(device)\n",
    "        logits = model(x)\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        idx = torch.multinomial(probs, num_samples=1).item()\n",
    "        new_name.append(decoder[idx])\n",
    "        context = context[1:] + [idx]\n",
    "        if idx == end_id:\n",
    "            break\n",
    "\n",
    "    return \"\".join(new_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77d393036879fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(5):\n",
    "    print(generate_name(\n",
    "        model=mlp,\n",
    "        context_size=MLPConfig.context_size,\n",
    "        decoder=idx2str,\n",
    "        end_id=str2idx[\".\"],\n",
    "        device=MLPConfig.device\n",
    "    ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
